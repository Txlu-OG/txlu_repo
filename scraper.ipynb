{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a897124d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'articles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviral hemorrhagic fever\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Search for specific keywords\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m,            \u001b[38;5;66;03m# Start date for articles\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msortBy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublishedAt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiKey\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9c40dfed39574e099b7f036dcbf30ed8\u001b[39m\u001b[38;5;124m'\u001b[39m         \u001b[38;5;66;03m# Your API key here\u001b[39;00m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m---> 12\u001b[0m articles \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m], article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m], article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'articles'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "params = {\n",
    "    'q': 'viral hemorrhagic fever',  # Search for specific keywords\n",
    "    'from': '2023-01-01',            # Start date for articles\n",
    "    'sortBy': 'publishedAt',\n",
    "    'apiKey': '9c40dfed39574e099b7f036dcbf30ed8'         # Your API key here\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "articles = response.json()['articles']\n",
    "\n",
    "for article in articles:\n",
    "    print(article['title'], article['description'], article['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b71338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed request. Status code: 426\n",
      "Response: {\"status\":\"error\",\"code\":\"parameterInvalid\",\"message\":\"You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-08-29, but you have requested 2023-01-01. You may need to upgrade to a paid plan.\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "params = {\n",
    "    'q': 'viral hemorrhagic fever',  # Search for specific keywords\n",
    "    'from': '2023-01-01',            # Start date for articles\n",
    "    'sortBy': 'publishedAt',\n",
    "    'apiKey': '9c40dfed39574e099b7f036dcbf30ed8'         # Your API key here\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    if 'articles' in response_data:\n",
    "        articles = response_data['articles']\n",
    "        for article in articles:\n",
    "            print(article['title'], article['description'], article['url'])\n",
    "    else:\n",
    "        print(\"No articles found. Response: \", response_data)\n",
    "else:\n",
    "    print(\"Failed request. Status code:\", response.status_code)\n",
    "    print(\"Response:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c40ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "articles = soup.find_all('h3', class_='gs-c-promo-heading__title')\n",
    "\n",
    "for article in articles:\n",
    "    print(article.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc3e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    title = article.text\n",
    "    link = article.find('a')['href']  # Extract the URL of the article\n",
    "    full_link = f\"https://www.bbc.com{link}\"  # Ensure it's a full URL\n",
    "\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"URL: {full_link}\")\n",
    "    print(\"-------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2089afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('bbc_articles.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Content'])  # Write header row\n",
    "\n",
    "    # Loop through articles and write the details into the CSV\n",
    "    for article in articles:\n",
    "        title = article.text\n",
    "        link = article.find('a')['href']\n",
    "        full_link = f\"https://www.bbc.com{link}\"\n",
    "\n",
    "        # Request the article page\n",
    "        article_page = requests.get(full_link)\n",
    "        article_soup = BeautifulSoup(article_page.text, 'html.parser')\n",
    "\n",
    "        # Extract article content\n",
    "        paragraphs = article_soup.find_all('p')\n",
    "        article_content = ' '.join([para.text for para in paragraphs])\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([title, full_link, article_content])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef68591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(client_id='YOUR_CLIENT_ID',\n",
    "                     client_secret='YOUR_CLIENT_SECRET',\n",
    "                     user_agent='YOUR_USER_AGENT')\n",
    "\n",
    "# Scraping posts from a subreddit (e.g., r/epidemiology)\n",
    "subreddit = reddit.subreddit('epidemiology')\n",
    "\n",
    "# Search for specific keywords in titles\n",
    "for post in subreddit.search('viral hemorrhagic fever', limit=100):\n",
    "    print(post.title, post.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13dc89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.who.int/news-room/articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "articles = soup.find_all('h2', class_='article__title')\n",
    "\n",
    "for article in articles:\n",
    "    print(article.text, article.find('a')['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d00a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 0 articles from Google News.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get Google News search results\n",
    "def scrape_google_news(query, from_date, to_date):\n",
    "    url = f'https://www.google.com/search?q={query}+after:{from_date}+before:{to_date}&tbm=nws'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for item in soup.find_all('div', class_='dbsr'):\n",
    "        title = item.find('div', class_='JheGif').get_text()\n",
    "        link = item.a['href']\n",
    "        snippet = item.find('div', class_='Y3v8qd').get_text()\n",
    "\n",
    "        articles.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'snippet': snippet\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Define query and date range\n",
    "search_query = 'viral hemorrhagic fever OR Lassa fever OR Ebola'\n",
    "from_date = '2017-06-01'\n",
    "to_date = '2023-06-30'\n",
    "\n",
    "# Scrape Google News\n",
    "articles = scrape_google_news(query=search_query, from_date=from_date, to_date=to_date)\n",
    "\n",
    "# Save results to a DataFrame and export as CSV\n",
    "df = pd.DataFrame(articles)\n",
    "df.to_csv('google_news_articles.csv', index=False)\n",
    "\n",
    "print(f\"Scraped {len(df)} articles from Google News.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a2097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get Google News search results\n",
    "def scrape_google_news(query, from_date, to_date):\n",
    "    # Modify the search query to include the date range\n",
    "    url = f'https://www.google.com/search?q={query}+after:{from_date}+before:{to_date}&tbm=nws'\n",
    "    \n",
    "    # Set user-agent to avoid being blocked\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    # Send the request to Google\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract articles from the HTML\n",
    "    articles = []\n",
    "    \n",
    "    for result in soup.select('.dbsr'):\n",
    "        try:\n",
    "            title = result.select_one('.nDgy9d').get_text()  # Update with correct class\n",
    "            link = result.find('a')['href']\n",
    "            snippet = result.select_one('.Y3v8qd').get_text()  # Update with correct class\n",
    "            \n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'snippet': snippet\n",
    "            })\n",
    "        except AttributeError as e:\n",
    "            # Handle the case where the article doesn't have the expected structure\n",
    "            print(f\"Skipping an article due to missing data: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Define query and date range\n",
    "search_query = 'viral hemorrhagic fever OR Lassa fever OR Ebola'\n",
    "from_date = '2017-06-01'\n",
    "to_date = '2023-06-30'\n",
    "\n",
    "# Scrape Google News\n",
    "articles = scrape_google_news(query=search_query, from_date=from_date, to_date=to_date)\n",
    "\n",
    "# Save results to a DataFrame and export as CSV\n",
    "if articles:\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv('google_news_articles.csv', index=False)\n",
    "    print(f\"Scraped {len(df)} articles from Google News.\")\n",
    "else:\n",
    "    print(\"No articles found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35983ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=561a83422bfeabbd2262e57730afa297f2b2b8385b58cb6584122a08a2e88001\n",
      "  Stored in directory: /Users/m1/Library/Caches/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a3ddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 98 articles from Google News RSS.\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch Google News RSS feed\n",
    "def fetch_google_news_rss(query):\n",
    "    # URL-encode the search query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    \n",
    "    # Modify the RSS URL to use the encoded query\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en\"\n",
    "    \n",
    "    # Parse the RSS feed\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        articles.append({\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "            'published': entry.published,\n",
    "            'summary': entry.summary\n",
    "        })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Define query (search for keywords related to your topic)\n",
    "search_query = 'Lassa fever OR Ebola OR viral hemorrhagic fever'\n",
    "\n",
    "# Fetch articles\n",
    "articles = fetch_google_news_rss(query=search_query)\n",
    "\n",
    "# Check if we have articles\n",
    "if articles:\n",
    "    # Save to a DataFrame and export as CSV\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv('google_news_rss_articles.csv', index=False)\n",
    "    print(f\"Fetched {len(df)} articles from Google News RSS.\")\n",
    "else:\n",
    "    print(\"No articles found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1291c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 35 articles from Google News RSS.\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch Google News RSS feed\n",
    "def fetch_google_news_rss(query):\n",
    "    # URL-encode the search query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    \n",
    "    # Modify the RSS URL to use the encoded query\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en\"\n",
    "    \n",
    "    # Parse the RSS feed\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    # Define the date range for filtering articles (from January 2017 to June 2023)\n",
    "    start_date = datetime(2017, 1, 1)\n",
    "    end_date = datetime(2023, 6, 30)\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        # Convert the published date to a datetime object\n",
    "        published_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')\n",
    "        \n",
    "        # Check if the article is within the specified date range\n",
    "        if start_date <= published_date <= end_date:\n",
    "            # Extract the publisher/source (if available)\n",
    "            publisher = entry.get('source', {}).get('title', 'Unknown Publisher')\n",
    "\n",
    "            # Append relevant details to the articles list\n",
    "            articles.append({\n",
    "                'title': entry.title,\n",
    "                'link': entry.link,\n",
    "                'published': published_date,\n",
    "                'publisher': publisher,\n",
    "                'summary': entry.summary\n",
    "            })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Define query (search for keywords related to your topic)\n",
    "search_query = 'Lassa fever OR Ebola OR viral hemorrhagic fever'\n",
    "\n",
    "# Fetch articles\n",
    "articles = fetch_google_news_rss(query=search_query)\n",
    "\n",
    "# Check if we have articles\n",
    "if articles:\n",
    "    # Save to a DataFrame and export as CSV\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv('google_news_rss_articles_filtered.csv', index=False)\n",
    "    print(f\"Fetched {len(df)} articles from Google News RSS.\")\n",
    "else:\n",
    "    print(\"No articles found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe9486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 42 articles from Google News RSS.\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch Google News RSS feed\n",
    "def fetch_google_news_rss(query):\n",
    "    # URL-encode the search query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    \n",
    "    # Modify the RSS URL to use the encoded query\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en\"\n",
    "    \n",
    "    # Parse the RSS feed\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    # Define the date range for filtering articles (from January 2017 to June 2023)\n",
    "    start_date = datetime(2017, 1, 1)\n",
    "    end_date = datetime(2024, 9, 30)\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        # Convert the published date to a datetime object\n",
    "        published_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')\n",
    "        \n",
    "        # Check if the article is within the specified date range\n",
    "        if start_date <= published_date <= end_date:\n",
    "            # Extract the publisher/source (if available)\n",
    "            publisher = entry.get('source', {}).get('title', 'Unknown Publisher')\n",
    "\n",
    "            # Append relevant details to the articles list\n",
    "            articles.append({\n",
    "                'title': entry.title,\n",
    "                'link': entry.link,\n",
    "                'published': published_date,\n",
    "                'publisher': publisher,\n",
    "                'summary': entry.summary\n",
    "            })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Expanded query to include more VHF-related keywords\n",
    "search_query = (\n",
    "    'Lassa fever OR Ebola OR viral hemorrhagic fever OR Marburg virus OR '\n",
    "    'Crimean-Congo hemorrhagic fever OR Hantavirus OR Dengue OR Rift Valley fever OR '\n",
    "    'Filovirus OR Arenavirus OR Hemorrhagic syndrome OR Severe fever with thrombocytopenia syndrome'\n",
    ")\n",
    "\n",
    "# Fetch articles\n",
    "articles = fetch_google_news_rss(query=search_query)\n",
    "\n",
    "# Check if we have articles\n",
    "if articles:\n",
    "    # Save to a DataFrame and export as CSV\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv('google_news_rss_articles_filtered_expanded.csv', index=False)\n",
    "    print(f\"Fetched {len(df)} articles from Google News RSS.\")\n",
    "else:\n",
    "    print(\"No articles found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e51159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
